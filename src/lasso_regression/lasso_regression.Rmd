---
title: "Lasso Regression"
author: "Philipp Wyss"
date: "2024-04-23"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    smooth_scroll: false
    df_print: paged
    code_folding: hide

    highlight: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      fig.width=9, fig.height=5,
                      fig.path='../../results/fig_lasso_regression/',
                      cache = TRUE,
                      cache.path = './cache/')

# Load libraries
library(tidyverse)
library(lubridate)
library(glmnet)
library(zoo)
library(xts)
library(forecast)
library(tseries)
#library(glmnet)
```

# Load Data
Loading the hourly aggregated dataset.

```{r select location}
# Select the location
select_standort <- 1037
```


```{r read data 2022}
dat2022 <- read.csv('../../results/df_agg_hourly_2022.csv')
dat2022 <- dat2022 %>% filter(Standort == select_standort)
```

```{r read data 2023}
dat2023 <- read.csv('../../results/df_agg_hourly_2023.csv')
dat2023 <- dat2023 %>% filter(Standort == select_standort)
```


Next we decide which location to use. We will use the location with the most data available.
```{r select location}
# Combine the datasets
dat <- rbind(dat2022, dat2023)

# Check the time range of the data
range(dat[dat$Standort == select_standort, ]$Datetime)
```

# Data Preperation

**Attention**: Most data preperation is taking place in `üìÅ ./src/data_preparation.ipynb`.

What we are doing here:
1. Cast Date and Time Variables into proper datatypes
2. Calculate the total traffic (`bike_tot` = `VELO_IN` + `VELO_OUT`)
3. Filter for `Standort == 1037` (Hardbr√ºcke S√ºd (Seite HB))
4. Select only variables that are of interest

```{r data preperation}
prep <- dat %>% mutate(
  # Cast Date & Datetime to proper Timestamps using libridate
  Date = lubridate::ymd(Date),
  Datetime = lubridate::ymd_hm(Datetime),
  Weekday = weekdays(Date),
  # Convert Time to integer
  Time = as.integer(substr(Time, 1, 2)),
) %>%
  # Filter for Standort 20 (Milit√§rbr√ºcke / Langstrasse)
  filter(Standort == 1037) %>%
  # Select only variables that are of interest
  select(Standort,
         Date,
         Time,
         Datetime,
         Weekday,
         # Actual measurements
         bike_tot,
         ped_tot,
         # Weather variables
         Hr...Hr.,
         RainDur..min.,
         StrGlo..W.m2.,
         T...C.,
         WD....,
         WVs..m.s.,
         WVv..m.s.,
         p..hPa.,
         # Additional lacations descriptive variables
         bezeichnung) %>%
  # Transform Weekday to ordered factor starting with Monday
  mutate(Weekday = factor(Weekday, 
                          levels = c("Montag", "Dienstag", 
                                     "Mittwoch", "Donnerstag", 
                                     "Freitag", "Samstag", "Sonntag"))) %>%
  # Create new month variable
  mutate(Month = lubridate::month(Date, label = TRUE))

# Clean Up and save some space
rm(list = c("dat2022", "dat2023"))

# Check the first few rows
prep %>% 
  select(Standort, bezeichnung, Weekday, Date, Time, bike_tot, ped_tot) %>%
  head()
```


# EDA
Visualizing the data to get a better understanding of the data.

**Attention**: All plots and images generated here will be saved in `üìÅ ./results/fig_lasso_regression/`.

## Timeseries


```{r rolling mean hourly traffic}
prep %>%
  head(48) %>%
  # Calculate the rolling mean
  mutate(rolling_mean = zoo::rollmean(bike_tot, k = 3, fill = NA)) %>%
  # Calculate the rolling std
  mutate(rolling_std = zoo::rollapply(bike_tot, width = 3, FUN = sd, fill = NA)) %>%
  # Plot the timeseries
  ggplot(aes(x = Datetime, y = bike_tot)) +
  geom_point() +
  geom_line(aes(x = Datetime, y = rolling_std, colour = "rolling std")) +
  #geom_ribbon(aes(ymin = rolling_mean - rolling_std, ymax = rolling_mean + rolling_std), fill = "darkgreen", alpha = 0.2) +
  geom_line(aes(x = Datetime, y = rolling_mean, colour = "rolling mean")) +
  scale_color_manual(name = "", values = c("rolling std" = "darkgreen", "rolling mean" = "red")) +
  theme_classic() +
  theme(legend.position = "bottom") + 
  ylab("Velo Traffic") +
  ggtitle(paste("Bike traffic by hour at", prep$bezeichnung[1], "by day for 1. & 2. January"), subtitle = "Rolling mean and standard deviation for all observations")
```

Next we aggregate the data on a daily basis an plot the whole year -> is there any trend?
```{r rolling mean daily traffic}
# Calculate the rolling mean
prep %>%
  group_by(Date) %>%
  summarise(Traffic = sum(bike_tot)) %>%
  # Calculate the rolling mean
  mutate(rolling_mean = zoo::rollmean(Traffic, k = 7, fill = NA)) %>%
  # Calculate the rolling std
  mutate(rolling_std = zoo::rollapply(Traffic, width = 7, FUN = sd, fill = NA)) %>%
  # Plot the timeseries
  ggplot(aes(x = Date, y = Traffic)) +
  geom_point() +
  geom_line(aes(x = Date, y = rolling_std, colour = "rolling std")) +
  #geom_ribbon(aes(ymin = rolling_mean - rolling_std, ymax = rolling_mean + rolling_std), fill = "darkgreen", alpha = 0.2) +
  geom_line(aes(x = Date, y = rolling_mean, colour = "rolling mean")) +
  scale_color_manual(name = "", values = c("rolling std" = "darkgreen", "rolling mean" = "red")) +
  theme_classic() +
  theme(legend.position = "bottom") + 
  ylab("Velo Traffic") +
  ggtitle(paste("Daily Bike traffic at", prep$bezeichnung[1], "by day from Jan - Dec 2023"), subtitle = "Rolling mean and standard deviation for all observations")

```

```{r custom weekday colours}
weekday_colours <- c(
      "Montag" = "#DFE2E8",
      "Dienstag" = "#AEB7C6",
      "Mittwoch" = "#9EA8BA",
      "Donnerstag" = "#8D99AE",
      "Freitag" = "#808EA5",
      "Samstag" = "#F47382",
      "Sonntag" = "#D90429")
```


```{r plot timeseries daily traffic}
prep %>%
  # Group by Date
  group_by(Date, Weekday) %>%
  # Summarize the total traffic
  summarise(Traffic = sum(bike_tot)) %>%
  # Plot the timeseries
  ggplot(aes(x = Date, y = Traffic, col = Weekday)) + 
  geom_point() +
  geom_line() +
  geom_vline(xintercept = date("2023-01-01"), linetype='dashed') +
  theme_classic() +
  ylab("Velo Traffic") +
  ggtitle(paste("Bike traffic at", prep$bezeichnung[1], "by day from Jan - Dec 2023")) +
  theme(legend.position = "bottom") + 
  # Manualy set color scale
  scale_colour_manual(
    values = weekday_colours
  )
```


## Boxplot
```{r boxplot monthly traffic}
prep %>%
  # Select only the variables we need
  select(Month, Weekday, bike_tot) %>%
  # Filter only Jan & Feb
  filter(Month %in% c("Apr", "Jul", "Nov", "Dez")) %>%
  # Plot the boxplot
  ggplot(aes(x = Month, y = bike_tot, col = Weekday)) +
  geom_boxplot() +
  theme_classic() +
  ylab("Velo Traffic") +
  ggtitle(paste("Monthly bike traffic at", prep$bezeichnung[1], "by month from Jan - Dec")) +
    theme(legend.position = "bottom") + 
    # Manualy set color scale
  scale_colour_manual(
    values = weekday_colours
  )
```


## Correlation Matrix
Relative humidity and temperature are highly correlated. This is expected as the relative humidity is a function of the temperature. We will remove the relative humidity from the model.
```{r correlation matrix}
# Calculate the correlation matrix
cor_matrix <- prep %>%
  select(bike_tot, Hr...Hr., RainDur..min., StrGlo..W.m2., T...C., WD...., WVs..m.s., WVv..m.s., p..hPa.) %>%
  cor()

# Plot the correlation matrix
corrplot::corrplot(cor_matrix, method = "color", type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
```


## Plot RainDur

```{r plot raindur}
# Plot RainDur and StrGlo as a function of Datetime
# First transfrom the data to long format
# Then plot the values as bars and add facet_wrap by the variable
prep %>%
  head(200) %>%
  pivot_longer(cols = c(RainDur..min., StrGlo..W.m2., Hr...Hr.), 
              names_to = "Variable", 
              values_to = "Value") %>%
  ggplot(aes(x = Datetime, y = Value)) +
  geom_bar(stat = "identity") +
  facet_wrap(vars(Variable), ncol = 1) +
  theme_classic() +
  ylab("Value") +
  ggtitle(paste("Rain duration and global radiation at", prep$bezeichnung[1], "by day in January")) +
  theme(legend.position = "bottom")
```

# Time Series Analysis
The log-transformed timeseries looks lot more **stationary**.

```{r create time series object}
# Create a time series object using xts
velo_ts <- xts::xts(x = prep$bike_tot, 
               order.by = prep$Datetime, 
               frequency = 24 * length(unique(prep$Date)))

# Create a log transformed time-series
velo_log_ts <- log(velo_ts + 1)

# Merge the series
velo_ts_merge <-  merge.xts(velo_ts, velo_log_ts)

# Plot the time series
plot.xts(velo_ts_merge, col = c("blue", "red"),
         multi.panel = TRUE, yaxis.same = FALSE,
         main = "Original vs Log-transformed series")
```

To remove the seasonal pattern, we might want to use a seasonally-adjusted time series. Otherwise, we could also create a dummy variable for the seasonal period (that is, a variable that characterises the weekends).

We will be using the manualy created variable `weekday` (ordered factor) for compensating for the weekends.

```{r}
# Select only 2022 data for training the model
ts_select <- prep %>% filter(year(Date) == 2022)

bike_ts <- ts(data = ts_select$bike_tot, 
              start = min(ts_select$bike_tot), 
              end = max(ts_select$bike_tot), 
              frequency = max(week(ts_select$Date)))
```

### Decomposed data

```{r decompose bike_ts}
# Decompose
bike_ts_decompose <-  decompose(bike_ts)
plot(bike_ts_decompose)
```

```{r autocorrelation}
# Calculate Autocorrelation (Absolute Values)
cat(paste(
"###################################\nTOTAL AUTOCORRELATION (ABSOLUTE VALUES)\n###################################",
"ADITTIVE MODEL = ", 
    round(sum(abs(acf(na.omit(bike_ts_decompose$random), plot = F)$acf)),2),
"\n\n###################################\nSUM OF RESIDUALS (ABSOLUTE VALUES)\n###################################",
"ADITTIVE MODEL = ", 
    round(sum(abs(scale(bike_ts_decompose$random)), na.rm=T),2)
))
```

### Seasonal adjusted data

```{r adjusted data}
bike_detrended <- bike_ts - bike_ts_decompose$trend

bike <- ts.intersect(bike_ts, bike_detrended)

plot.ts(bike[1:50,],
        plot.type = "single",
        col = c("red", "blue"),
        main = "Original (red) and Seasonally Adjusted Series (blue)")
```

The ACF of an autoregressive process typically shows a slow and gradual decay in autocorrelation over time.

```{r}
acf(na.omit(bike_ts_decompose$random))
```

The PACF of an autoregressive process shows a peak in correspondence with the order of the model. In the case of an AR(1) the peak is at time 1.

```{r}
pacf(na.omit(bike_ts_decompose$random)) # AR(p) model: the PACF vanishes after p lags
```
Moving Average (MA) model?
The PACF of a MA process shows an up-and-down movement and does not shut off, but instead tapers toward 0 in some manner.
```{r}
library(forecast)

arima_fit <- auto.arima(bike_ts)

arima_fit
```

```{r}
# remove the trend from a multiplicative model
bike_ts_decompose_mult <- decompose(bike_ts, type="multiplicative")
trend_component <- bike_ts_decompose_mult$trend
detrended <- bike_ts/trend_component

par(mfrow=c(1,2))
plot.ts(bike_ts, col = "blue", main = "Original series")
plot.ts(detrended, col = "blue",  
        main = "Detrended series", 
        ylab = "Detrended values")
```



```{r}
ar_3 <- ar(bike_ts, order.max = 3)
ar_3

ar_3 %>%
forecast(h = 10) %>%
autoplot() + autolayer(bike_ts) +
theme_bw() +
scale_color_manual(name = " ", values = c("blue"), labels = c("Sample"))
```



```{r}
auto_arima_model_2 <- auto.arima(bike_ts)
print(auto_arima_model_2)
```

```{r}
auto_arima_model_2 %>%
forecast(h = 40) %>%
autoplot() + theme_bw() +
scale_color_manual(name = " ", values = c("blue"), labels = c("Sample"))
```



### Augmented Dickey-Fuller Test (ADF)
```{r}
# original ts
adf.test(velo_ts)
```

```{r}
# Log transformed ts
adf.test(velo_log_ts)
```



### ACF
The ACF statistic measures the correlation between xt and xt+k where k is the number of lead periods into the future. It measures the correlation between any two points based on a given interval. It is not strictly equivalent to the Pearson product moment correlation. In R, ACF is calculated and visualized with the function ‚Äúacf‚Äù.

```{r autocorrelation function}
acf(velo_ts)
```

```{r partial autocorrelation function}

```


## Linear Model



```{r linear regression residuals ,fig.width=9, fig.height=10}
# Build multivariate linear model
lm_velo <- prep %>% 
  filter(year(Datetime) == 2022) %>%
  lm(bike_detrended ~ Weekday + Time + Month + Hr...Hr. + RainDur..min. + StrGlo..W.m2. + T...C. + WD.... + WVs..m.s. + WVv..m.s. + p..hPa., data = .)

par(mfrow = c(2,2))
plot(lm_velo)
```

```{r}
summary(lm_velo)
```

```{r predict}
newx <- prep %>% 
          filter(year(Datetime) == 2023) %>%
          select(bike_tot, Date, Weekday, Time,  Month,  Hr...Hr., RainDur..min., StrGlo..W.m2., T...C., WD...., WVs..m.s., WVv..m.s., p..hPa.)

pred <- predict(lm_velo, newx = newx %>% select(-bike_tot, Date))
```

```{r pred vs real}
plot(newx$Date, newx$bike_tot, type = "l")
lines(newx$Date, pred[1:8758], type = "l", col = "red")
```

# Vector Autoregressive Model


```{r}
library(vars)
```

The function to fit the model is VAR, and the number of lags is specified using the p parameter.

```{r VARselect}
vars::VARselect(newx %>% dplyr::select(bike_tot, Time, T...C.))
```

```{r}
var_fit <- vars::VAR(newx %>% dplyr::select(bike_tot, Time, T...C.), p = 10)
#summary(var_fit)
```


```{r}
plot(vars::stability(var_fit))
```

```{r predict}
pred <- predict(var_fit, newx = newx %>% dplyr::select(-bike_tot))
```

```{r pred vs real}
plot(newx$Date, newx$bike_tot, type = "l")
lines(newx$Date, pred$fcst$bike_tot, type = "l", col = "red")
```
