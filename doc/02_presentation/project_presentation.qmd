---
title: "Predicting Bicycle Traffic in Zurich"
author: "D. Bruelhart, M. Krähenbühl, P. Wyss and J. Tschanz"
institute: "University of Zurich"
format: 
  revealjs:
    theme: default
    logo: figs/UZHLogo.png
    footer: "Predicting Bicycle Traffic in Zurich | 18.05.2024"
    navigation: progress
editor: visual
---

## Table of contents

1. Introduction
2. Exploratory Data Analysis
3. Traffic modelling
4. Conclusion

## Introduction

::: columns
::: {.column width="60%"}
-   Increase use of bike and foot traffic

-   Development of public bike in cities

-   Need for such companies to provide enough

-   Impact of the weather on the traffic
:::

::: {.column width="40%"}
![](figs/web_zurich_opernhaus_velo_zt_16620_1600x900_01.jpg){fig-align="right"}
:::
:::

## Main Questions

-   How does weather and season affect foot and bike traffic?

-   is it possible to make predictions ?

## Data

::: panel-tabset
### Traffic

[Automated foot and bike countings (every 15min)](https://data.stadt-zuerich.ch/dataset/ted_taz_verkehrszaehlungen_werte_fussgaenger_velo) from [stadt-zuerich.ch](https://stadt-zuerich.ch)

- **Contains** : `ID of the location`, `Datetime`, `Velo_in`, `Velo_out`

### Metadata
Metadata for automated foot and bike traffic counting locations from [stadt-zuerich.ch](https://stadt-zuerich.ch)

- **Contains** : `ID of the location`, `Description`, `geometry`

### Weather

[Hourly meteo data in Zurich](https://opendata.swiss/en/dataset/stundlich-aktualisierte-meteodaten-seit-1992) from [opendata.swiss](https://opendata.swiss)

-   **Contains** : `Date`, `location`, `Air pressure (hPa)`, `Rain precipitation duration (min)`, `Temperature (C°)`, `global radiation (W/m2)`, `Relative humidity (%Hr)`, `Wind direction`

### Population
[Population Data - City of Zurich](https://opendata.swiss) from [opendata.swiss](https://opendata.swiss)

- **Contains** : `Year` and `Economic resident population of the city of Zurich by year`
:::

## Data: Measurement Locations

![](figs/citymap_marked.png){fig-align="center"}

## Exploratory Data Analysis: Traffic

```{r import libs, echo = FALSE, include = FALSE}
library(tidyverse)
library(lubridate)
library(caret)
library(glmnet)
```

```{r read and prep data}
# Select the location
select_standort <- 732

to_select <- c("Datetime",
               "Time",
               "Date", 
               "bezeichnung", 
               "bike_tot",
               "Hr...Hr.", 
               "RainDur..min.", 
               "StrGlo..W.m2.", 
               "T...C.", 
               "WD....", 
               "WVs..m.s.", 
               "WVv..m.s.", 
               "p..hPa.")

# 2022
dat2022 <- read.csv('../../results/df_agg_hourly_2022.csv')
dat2022 <- dat2022 %>% filter(Standort == select_standort) %>% 
  select(all_of(to_select))

# 2023
dat2023 <- read.csv('../../results/df_agg_hourly_2023.csv')
dat2023 <- dat2023 %>% filter(Standort == select_standort) %>% 
  select(all_of(to_select))

# Bind
dat <- rbind(dat2022, dat2023)
# Cleanup
rm(list = c("dat2022", "dat2023"))

# Mutate types
dat <- dat %>% mutate(Date = lubridate::as_date(Date),
                      Weekday = weekdays(Date)) %>%
  # Transform Weekday to ordered factor starting with Monday
  mutate(Weekday = factor(Weekday, 
                          levels = c("Montag", "Dienstag", 
                                     "Mittwoch", "Donnerstag", 
                                     "Freitag", "Samstag", "Sonntag")),
         Date = lubridate::ymd(Date),
         Datetime = lubridate::ymd_hm(Datetime),
         Month = lubridate::month(Date, label = TRUE),
         Time = factor(as.integer(substr(Time, 1, 2))))

colnames(dat) <- c("Datetime",
                   "Time",
               "Date", 
               "bezeichnung", 
               "bike_tot",
               "Hr [%Hr]", 
               "RainDur [min]", 
               "StrGlo [W/m2]", 
               "T [°C]", 
               "WD [°]", 
               "WVs [m/s]", 
               "WVv [m/s]", 
               "p [hPa]",
               "Weekday",
               "Month")
```

::: panel-tabset
### Timeseries

```{r Trendanalysis, warning = FALSE}
dat %>%
  select(Date, bike_tot) %>%
  group_by(Date) %>%
  summarise(Traffic = sum(bike_tot)) %>%
  # Calculate the rolling mean
  mutate(rolling_mean = zoo::rollmean(Traffic, k = 7, fill = NA)) %>%
  # Calculate the rolling std
  mutate(rolling_std = zoo::rollapply(Traffic, width = 7, FUN = sd, fill = NA)) %>%
  ggplot(aes(x = Date, y = Traffic)) +
  geom_point(pch = 20, col = "lightgrey") +
  geom_line(col = "lightgrey") + 
  geom_line(aes(x = Date, y = rolling_std, colour = "rolling std")) +
  geom_vline(xintercept = date("2023-01-01"), linetype='dashed') +
  #geom_ribbon(aes(ymin = rolling_mean - rolling_std, ymax = rolling_mean + rolling_std), fill = "darkgreen", alpha = 0.2) +
  geom_line(aes(x = Date, y = rolling_mean, colour = "rolling mean")) +
  scale_color_manual(name = "", values = c("rolling std" = "darkgreen", "rolling mean" = "red")) +
  theme_classic() +
  theme(legend.position = "bottom") + 
  ylab("Bike Traffic") +
  ggtitle(paste("Bike traffic at B:", dat$bezeichnung[1], " [2022 - 2023]"))
```

### Timeseries by Weekday

```{r custom weekday colours, echo = FALSE, include = FALSE}
weekday_colours <- c(
      "Montag" = "#DFE2E8",
      "Dienstag" = "#AEB7C6",
      "Mittwoch" = "#9EA8BA",
      "Donnerstag" = "#8D99AE",
      "Freitag" = "#808EA5",
      "Samstag" = "#F47382",
      "Sonntag" = "#D90429")
```

```{r plot timeseries daily traffic}
dat %>%
  select(Date, bike_tot, Weekday) %>%
  # Group by Date
  group_by(Date, Weekday) %>%
  # Summarize the total traffic
  summarise(Traffic = sum(bike_tot)) %>%
  # Plot the timeseries
  ggplot(aes(x = Date, y = Traffic, col = Weekday)) + 
  geom_point(pch = 20) +
  geom_line() +
  geom_vline(xintercept = date("2023-01-01"), linetype='dashed') +
  theme_classic() +
  ylab("Bike Traffic") +
  ggtitle(paste("Bike traffic at B:", dat$bezeichnung[1], " [2022 - 2023]")) +
  theme(legend.position = "bottom") + 
  # Manualy set color scale
  scale_colour_manual(
    values = weekday_colours
  )
```
:::

### Boxplot by Weekday

```{r boxplot by weekday}
dat %>%
  # Select only the variables we need
  select(Month, Weekday, bike_tot) %>%
  # Filter only Jan & Feb
  filter(Month %in% c("Apr", "Jul", "Nov", "Dez")) %>%
  # Plot the boxplot
  ggplot(aes(x = Month, y = bike_tot, col = Weekday)) +
  geom_boxplot() +
  theme_classic() +
  ylab("Bike Traffic") +
  ggtitle(paste("Bike traffic at B:", dat$bezeichnung[1], " [2022 - 2023]")) +
    theme(legend.position = "bottom") + 
    # Manualy set color scale
  scale_colour_manual(
    values = weekday_colours
  )
```

## Exploratory Data Analysis: Weather

::: panel-tabset
### Boxplot

```{r boxplot all variables}
dat %>%
  pivot_longer(cols = c(bike_tot, "Hr [%Hr]", "RainDur [min]", "StrGlo [W/m2]", 
        "T [°C]", "WD [°]", "WVs [m/s]", "WVv [m/s]", "p [hPa]"), 
              names_to = "Variable", 
              values_to = "Value") %>%
  ggplot(aes(x = Variable, y = Value)) +
  geom_boxplot() +
  theme_classic() +
  ggtitle("Boxplot of weather variables + bike_tot") +
  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))
```

### Weather

```{r plot weather timeseries}
# Plot RainDur and StrGlo as a function of Datetime
# First transfrom the data to long format
# Then plot the values as bars and add facet_wrap by the variable
dat %>%
  head(200) %>%
  pivot_longer(cols = c("RainDur [min]", "StrGlo [W/m2]", "Hr [%Hr]", "T [°C]"), 
              names_to = "Variable", 
              values_to = "Value") %>%
  ggplot(aes(x = Datetime, y = Value)) +
  geom_bar(stat = "identity") +
  facet_wrap(vars(Variable), ncol = 1) +
  theme_classic() +
  ylab("Value") +
  ggtitle(paste("Weather variables at", dat$bezeichnung[1], "by day in January")) +
  theme(legend.position = "bottom")
```

### Correlation

```{r correlation matrix}
# Calculate the correlation matrix
cor_matrix <- dat %>%
  select(bike_tot, "Hr [%Hr]", "RainDur [min]", "StrGlo [W/m2]", 
        "T [°C]", "WD [°]", "WVs [m/s]", "WVv [m/s]", "p [hPa]") %>%
  cor()

# Plot the correlation matrix
corrplot::corrplot(cor_matrix, method = "color", type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
```
:::

## Traffic Modelling

1.  Random Forest
2.  Linear Regression
3.  Lasso Regression
4.  Poisson Regression
5.  Negative binomial regression

## Random Forest

-   Use Data from 2020 - 2022 for training (all locations)
-   Predict bike traffic in 2023 as test

**Included Features**

-   `'Hr [%Hr]', 'RainDur [min]', 'T [°C]', 'WD [°]', 'WVv [m/s]', 'p [hPa]', 'Year', 'AnzBestWir', 'hour', 'month', 'day_of_week'`
-   Also including `'Standort'` for RF with all locations

## Random Forest: Results

::: panel-tabset
### All Locations

::: columns
::: {.column width="50%"}
![](figs/RF_All_locations_pred_vs_true.png){fig-align="center"}
:::

::: {.column width="50%"}
![](figs/RF_All_locations_variable_importance.png){fig-align="center"}
:::
:::

### Hardbrücke Nord
::: columns
::: {.column width="50%"}
![](figs/RF_Hardbruecke_nord_pred_vs_true.png){fig-align="center"}
:::

::: {.column width="50%"}
![](figs/RF_Hardbruecke_nord_variable_importance.png){fig-align="center"}
:::
:::

### Top 5 Features
::: columns
::: {.column width="50%"}
![](figs/RF_5features_Hardbruecke_nord_pred_vs_true.png){fig-align="center"}
:::

::: {.column width="50%"}
![](figs/RF_5features_Hardbruecke_nord_variable_importance.png){fig-align="center"}
:::
:::

:::

## Random Forest: Grid Search

- 50, 100, 200, 500, 1000, 2000, 5000, 10000
- On the single location seen before
- Best parameter is 2000
- Only slightly better metrics

## Random Forest: Comparison

![](figs/Comparison.png){fig-align="center"}

## Regression

- Predicting bike traffic at a certain location
- Model traffic with different regression techniques
- Train on 2022 data and test on 2023

**Included Features**

- `'Hr [%Hr]', 'RainDur [min]', 'T [°C]', 'WD [°]', 'WVv [m/s]', 'p [hPa]', 'Year', 'AnzBestWir', 'hour', 'month', 'day_of_week'`, additional lag variables `lag_1` till `lag_7`


## Linear Regression
Is `bike_tot` normally distributed?

::: columns
::: {.column width="50%"}
```{r hist of bike_tot}
hist(dat$bike_tot, xlab = "bike_tot", main = "Histogram of bike_tot")
```
:::

::: {.column width="50%"}
```{r hist of sqrt}
hist(sqrt(dat$bike_tot), xlab = "sqrt(bike_tot)", main = "Histogram of sqrt(bike_tot)")
```
:::
```{r Shapiro-Wilk}
set.seed(123)
# Test for normality
shapiro <- shapiro.test( x = sample(sqrt(dat$bike_tot), size = 5000, replace = F) )
if(shapiro$p.value < 0.005){
  pvalue <- "< 0.005"
}
```

Shapiro-Wilk normality test for normality reflecting significant departure from normality assumption with p-value `r pvalue`.

:::


## Linear Regression

```{r Create lagged variables}
# Create lagged variables
dat <- dat %>%
  mutate(lag_1 = dplyr::lag(bike_tot, n=1),
         lag_2 = dplyr::lag(bike_tot, n=2),
         lag_3 = dplyr::lag(bike_tot, n=3),
         lag_4 = dplyr::lag(bike_tot, n=4),
         lag_5 = dplyr::lag(bike_tot, n=5),
         lag_6 = dplyr::lag(bike_tot, n=6),
         lag_7 = dplyr::lag(bike_tot, n=7))
```

```{r split test and train}
train <- dat %>% filter(Date < ymd("2023-01-01"))
test <- dat %>% filter(Date >= ymd("2023-01-01"))
```

```{r create linear model}
lm_hardbrücke <- train %>% lm(sqrt(bike_tot) ~ `Hr [%Hr]` + `RainDur [min]` + 
                              `StrGlo [W/m2]` + `T [°C]` + `WD [°]` + `WVs [m/s]`
                            + `WVv [m/s]` + `p [hPa]` + Weekday + Month +
                              lag_1 + lag_2 + lag_3 + lag_4 + lag_5 + lag_6 +
                              lag_7, data = .)
```

::: panel-tabset
### Model Statistics

```{r lm summary, echo = FALSE, include=FALSE}
sm <- summary(lm_hardbrücke)
sm$adj.r.squared
```
- $\sqrt{y_i} = \beta_0+\beta_1x_1+...+\beta_nx_n+\epsilon_i$
- Adjusted R-squard: `r round(sm$adj.r.squared,4)`
- Residual standard error: `r round(sd(sm$residuals),2)` on `r round(sm$df[2],0)` degrees of freedom

### Residuals

```{r lm residuals}
par(mfrow = c(2,2))
plot(lm_hardbrücke)
```


### Variable Importance

```{r lm Variable Importance}
lm_importance <- caret::varImp(lm_hardbrücke)
lm_importance <- tibble(Value = lm_importance[,1],
       Variable = rownames(lm_importance))

lm_importance <- lm_importance %>% arrange(across(Value, desc))

ggplot(lm_importance, aes(x = Variable, y = Value)) +
  geom_bar(stat = "identity", fill = "#808EA5") +
  coord_flip() +
  labs(x = "Variable", y = "Importance") +
  theme_classic() +
  ggtitle("Variable Importance") +
  scale_x_discrete(limits = rev(lm_importance$Variable))
```
:::

## Linear Regression: Prediction


::: panel-tabset
### Full Year
```{r lm predict}
predict_exclude <- c("Datetime", "Date", "bezeichnung", "bike_tot",
                       "bezeichnung")

pred <- predict(lm_hardbrücke, 
                newdata = test %>% select(-c(all_of(predict_exclude))),
                interval = "confidence")

pred <- cbind(pred, test[,c("Datetime", "bike_tot")])

pred %>% 
  ggplot(aes(x = Datetime, y = bike_tot, col = "true value")) + 
  geom_point(pch = 20) +
  geom_line() +
  geom_point(aes(x = Datetime, y = fit^2, col = "fitted value")) +
  geom_line(aes(x = Datetime, y = fit^2, col = "fitted value")) +
  ylab("Bike Traffic") +
  ggtitle(paste("Predicted Bike traffic at B:", dat$bezeichnung[1], " [2023]")) +
  theme_classic() +
  theme(legend.position = "bottom") + 
  # Manualy set color scale
  scale_colour_manual(
    values = c("#F47382", "#9EA8BA")
  )
```
### January
```{r lm predict january}
pred %>% head(250) %>%
  ggplot(aes(x = Datetime, y = bike_tot, col = "true value")) + 
  geom_point(pch = 20) +
  geom_line() +
  geom_point(aes(x = Datetime, y = fit^2, col = "fitted value")) +
  geom_line(aes(x = Datetime, y = fit^2, col = "fitted value")) +
  ylab("Bike Traffic") +
  ggtitle(paste("Predicted Traffic at B:", dat$bezeichnung[1], " [2023]")) +
  theme_classic() +
  theme(legend.position = "bottom") + 
  # Manualy set color scale
  scale_colour_manual(
    values = c("#F47382", "#9EA8BA")
  )
```


### Performance
```{r lm calculate rmse}
# Calculate model performance - Residual mean squared error
lm_hardbrücke_rmse <- sqrt(mean((test$bike_tot - pred$fit)^2))
```
Residual mean squared error : `r round(lm_hardbrücke_rmse,2)`

::: 

## Lasso Regression


```{r lasso regression, message=FALSE}
set.seed(123)

# Create a model matrix of the predictors
X <- model.matrix(bike_tot ~ `Hr [%Hr]` + `RainDur [min]` + 
                  `StrGlo [W/m2]` + `T [°C]` + `WD [°]` + `WVs [m/s]` + 
                  `WVv [m/s]` + `p [hPa]` + Weekday + Month +
                  lag_1 + lag_2 + lag_3 + lag_4 + lag_5 + lag_6 +
                  lag_7, data = train[8:dim(train)[1],])

# Create the response variable
y = train[8:dim(train)[1],]$bike_tot

cvfit <- glmnet::cv.glmnet(x = X, 
                           y = y, 
                           type.measure = "mse", 
                           alpha = 1, # Lasso
                           family = "gaussian",
                           nfolds = 20)

# Predict the test set
X_test <- model.matrix(bike_tot ~ `Hr [%Hr]` + `RainDur [min]` + 
                  `StrGlo [W/m2]` + `T [°C]` + `WD [°]` + `WVs [m/s]` + 
                  `WVv [m/s]` + `p [hPa]` + Weekday + Month +
                  lag_1 + lag_2 + lag_3 + lag_4 + lag_5 + lag_6 +
                  lag_7, data = test)
y_test <- test$bike_tot

# Predict the test set
pred_lasso <- predict(cvfit, newx = X_test, s = cvfit$lambda.min)
pred_lasso <- cbind(pred_lasso, test[,c("Datetime", "bike_tot")])

# Calculate model performance - Residual mean squared error
lasso_hardbrucke_rmse <- sqrt(mean((test$bike_tot - pred_lasso$s1)^2))
```
::: panel-tabset

### Model Statistics

- 
- $\lambda_{\sigma}$ = `r round(cvfit$lambda.1se,2)` with `r cvfit$nzero[cvfit$lambda == cvfit$lambda.1se]` nonzero coefficients

### Cross-validation

```{r plot cvfit}
plot(cvfit)
```

### Variable Selection
```{r lasso variable selection}
lasso_selection <- tibble(Coef = coef(cvfit, s = cvfit$lambda.1se)[,1],
                          Variable = rownames(coef(cvfit, s = cvfit$lambda.min)))
# Filter out coefs that are 0
lasso_selection <- lasso_selection %>% filter(Coef < 0 | Coef > 0)

ggplot(lasso_selection, aes(x = Variable, y = Coef)) +
  geom_bar(stat = "identity", fill = "#808EA5") +
  coord_flip() +
  labs(x = "Variable", y = "Coefficient") +
  theme_classic() +
  ggtitle("Lasso Nonzero Coefficients")
```
:::

## Lasso Regression: Prediction

::: panel-tabset
### Full Year
```{r lasso predict}
pred_lasso %>% 
  ggplot(aes(x = Datetime, y = bike_tot, col = "true value")) + 
  geom_point(pch = 20) +
  geom_line() +
  geom_point(aes(x = Datetime, y = s1, col = "fitted value")) +
  geom_line(aes(x = Datetime, y = s1, col = "fitted value")) +
  ylab("Bike Traffic") +
  ggtitle(paste("Predicted Bike traffic at B:", dat$bezeichnung[1], " [2023]")) +
  theme_classic() +
  theme(legend.position = "bottom") + 
  # Manualy set color scale
  scale_colour_manual(
    values = c("#F47382", "#9EA8BA")
  )
```
### January
```{r lasso predict january}
pred_lasso %>% head(250) %>%
  ggplot(aes(x = Datetime, y = bike_tot, col = "true value")) + 
  geom_point(pch = 20) +
  geom_line() +
  geom_point(aes(x = Datetime, y = s1, col = "fitted value")) +
  geom_line(aes(x = Datetime, y = s1, col = "fitted value")) +
  ylab("Bike Traffic") +
  ggtitle(paste("Predicted Traffic at B:", dat$bezeichnung[1], " [2023]")) +
  theme_classic() +
  theme(legend.position = "bottom") + 
  # Manualy set color scale
  scale_colour_manual(
    values = c("#F47382", "#9EA8BA")
  )
```


### Performance
```{r lasso calculate rmse}
# Calculate model performance - Residual mean squared error
lasso_hardbrücke_rmse <- sqrt(mean((test$bike_tot - pred$fit)^2))
```
Residual mean squared error : `r round(lasso_hardbrücke_rmse,2)`

::: 

## Poisson Regression

::: columns
::: {.column width="40%"}

- Data suggests poisson distribution
- $\mu$ = `r round(mean(dat$bike_tot),0)`
- $\sigma^2$ = `r round(var(dat$bike_tot),0)`
:::

::: {.column width="60%"}
```{r histogram of bike_tot}
hist(dat$bike_tot, xlab = "Bike traffic per hour",
     main = dat$bezeichnung[1])
```
:::
:::

## Poisson Regression: Results

```{r poisson model}
# Create poisson regression model
set.seed(123)
fit_pois <- glm(bike_tot ~ `Hr [%Hr]` + `RainDur [min]` + 
                  `StrGlo [W/m2]` + `T [°C]` + `WD [°]` + `WVs [m/s]` + 
                  `WVv [m/s]` + `p [hPa]` + Weekday + Month +
                  lag_1 + lag_2 + lag_3 + lag_4 + lag_5 + lag_6 +
                  lag_7, 
                data = train,
                family = poisson(link="sqrt"))

# Predict
pred_pois <- predict(fit_pois, 
                     newdata = test %>% select(-c(all_of(predict_exclude))), 
                     type = "response")

pred_pois <- cbind(pred_pois, test[,c("Datetime", "bike_tot")])
```


::: panel-tabset
### Full Year
```{r poisson predict plot}
pred_pois %>% 
  ggplot(aes(x = Datetime, y = bike_tot, col = "true value")) + 
  geom_point(pch = 20) +
  geom_line() +
  geom_point(aes(x = Datetime, y = pred_pois, col = "fitted value")) +
  geom_line(aes(x = Datetime, y = pred_pois, col = "fitted value")) +
  ylab("Bike Traffic") +
  ggtitle(paste("Predicted Bike traffic at B:", dat$bezeichnung[1], " [2023]")) +
  theme_classic() +
  theme(legend.position = "bottom") + 
  # Manualy set color scale
  scale_colour_manual(
    values = c("#F47382", "#9EA8BA")
  )
```

### January
```{r poisson predict plot january}
pred_pois %>% head(250) %>%
  ggplot(aes(x = Datetime, y = bike_tot, col = "true value")) + 
  geom_point(pch = 20) +
  geom_line() +
  geom_point(aes(x = Datetime, y = pred_pois, col = "fitted value")) +
  geom_line(aes(x = Datetime, y = pred_pois, col = "fitted value")) +
  ylab("Bike Traffic") +
  ggtitle(paste("Predicted Traffic at B:", dat$bezeichnung[1], " [2023]")) +
  theme_classic() +
  theme(legend.position = "bottom") + 
  # Manualy set color scale
  scale_colour_manual(
    values = c("#F47382", "#9EA8BA")
  )
```


### Performance
```{r poisson rmse}
# Calculate model performance - Residual mean squared error
pois_hardbrücke_rmse <- sqrt(mean((pred_pois$bike_tot - pred_pois$pred_pois)^2))
```
Residual mean squared error : `r round(pois_hardbrücke_rmse,2)`

::: 


## Negative Binomial Regression

## Model Comparison

## Conclusion